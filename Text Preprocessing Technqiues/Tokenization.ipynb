{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\mahen\\anaconda\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\mahen\\anaconda\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\mahen\\anaconda\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\mahen\\anaconda\\lib\\site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: tqdm in c:\\users\\mahen\\anaconda\\lib\\site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\mahen\\anaconda\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello this is 'mahendra'. , and iam currently learning the NLP.,and by this NLP Knowledge iam going to create End to End Nlp applications.\n"
     ]
    }
   ],
   "source": [
    "corpus='''Hello this is 'mahendra'. , and iam currently learning the NLP.,and by this NLP Knowledge iam going to create End to End Nlp applications.'''\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "## sentence to Paragraph\n",
    "from nltk.tokenize import sent_tokenize # so this sent_tokenize iam going to use this package in order to convert the paragraph into sentence\n",
    "documents=sent_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello this is 'mahendra'.\n",
      ", and iam currently learning the NLP.,and by this NLP Knowledge iam going to create End to End Nlp applications.\n"
     ]
    }
   ],
   "source": [
    "for sentence in documents:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenization\n",
    "## paragraph-->words   # and we do perform this tokenaization from sentence or paragraph to words because we to focus on each words because each words has its difference importance and we really need to perform the preprocessing on top of it   \n",
    "## sentence -->words\n",
    "from nltk.tokenize import word_tokenize\n",
    "words=word_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'this', 'is', \"'mahendra\", \"'\", '.', ',', 'and', 'iam', 'currently', 'learning', 'the', 'NLP.', ',', 'and', 'by', 'this', 'NLP', 'Knowledge', 'iam', 'going', 'to', 'create', 'End', 'to', 'End', 'Nlp', 'applications', '.']\n"
     ]
    }
   ],
   "source": [
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'this', 'is', \"'mahendra\", \"'\", '.']\n",
      "[',', 'and', 'iam', 'currently', 'learning', 'the', 'NLP.', ',', 'and', 'by', 'this', 'NLP', 'Knowledge', 'iam', 'going', 'to', 'create', 'End', 'to', 'End', 'Nlp', 'applications', '.']\n"
     ]
    }
   ],
   "source": [
    "for sentence in documents:\n",
    "    print(word_tokenize(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'this',\n",
       " 'is',\n",
       " \"'\",\n",
       " 'mahendra',\n",
       " \"'.\",\n",
       " ',',\n",
       " 'and',\n",
       " 'iam',\n",
       " 'currently',\n",
       " 'learning',\n",
       " 'the',\n",
       " 'NLP',\n",
       " '.,',\n",
       " 'and',\n",
       " 'by',\n",
       " 'this',\n",
       " 'NLP',\n",
       " 'Knowledge',\n",
       " 'iam',\n",
       " 'going',\n",
       " 'to',\n",
       " 'create',\n",
       " 'End',\n",
       " 'to',\n",
       " 'End',\n",
       " 'Nlp',\n",
       " 'applications',\n",
       " '.']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize  # it helps us to even create tokens or consider punchuation marks ' ' as separate thing from corpus \n",
    "\n",
    "wordpunct_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'this',\n",
       " 'is',\n",
       " \"'mahendra'.\",\n",
       " ',',\n",
       " 'and',\n",
       " 'iam',\n",
       " 'currently',\n",
       " 'learning',\n",
       " 'the',\n",
       " 'NLP.',\n",
       " ',',\n",
       " 'and',\n",
       " 'by',\n",
       " 'this',\n",
       " 'NLP',\n",
       " 'Knowledge',\n",
       " 'iam',\n",
       " 'going',\n",
       " 'to',\n",
       " 'create',\n",
       " 'End',\n",
       " 'to',\n",
       " 'End',\n",
       " 'Nlp',\n",
       " 'applications',\n",
       " '.']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer  # as we can see in the above tokenization methods like word_punc_tokenize and word_tokenize we can clearly see that . is consider as separate token word in these methods but in thes method TreebankWordTokenizer we can clearly see only the last word of the sentence pulistop is consider as word token and before the sentences this method includes the . into the end of the particular word token \n",
    "\n",
    "tokenizer=TreebankWordTokenizer()\n",
    "tokenizer.tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
